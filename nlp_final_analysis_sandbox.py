# -*- coding: utf-8 -*-
"""NLP_FINAL_ANALYSIS_SANDBOX.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mWVg149-ID8Zg-iNsmKzvBFkLHGRe2Ms
"""

# Run this cell once to setup the session/environment
!rm -rf sample_data
!rm -rf fp-dataset-artifacts
!git clone https://github.com/dtsal3/fp-dataset-artifacts/

import os
os.chdir("/content/fp-dataset-artifacts")
!git pull 
os.chdir("/content/")



!pip install -r fp-dataset-artifacts/requirements.txt

!pip install checklist
!jupyter nbextension install --py --sys-prefix checklist.viewer
!jupyter nbextension enable --py --sys-prefix checklist.viewer
!python -m spacy download en_core_web_sm

import nltk
nltk.download('omw-1.4')

import checklist
import spacy
import itertools

import numpy as np
import torch
import transformers
from transformers import AutoModelForQuestionAnswering, AutoTokenizer

from checklist.test_suite import TestSuite
import checklist.editor
import checklist.text_generation
from checklist.test_types import MFT, INV, DIR
from checklist.expect import Expect
from checklist.test_suite import TestSuite
from checklist.perturb import Perturb
from transformers import pipeline

from datasets import Dataset
import datasets
squad_adv_addSent = datasets.load_dataset('squad_adversarial', 'AddOneSent')
nq_data = datasets.load_dataset("json",data_files="./fp-dataset-artifacts/datasets/nq_train.json", field='data')
squad = datasets.load_dataset('squad')
squad_v2 = datasets.load_dataset('squad_v2')


editor = checklist.editor.Editor()
editor.tg

model_dir = "./fp-dataset-artifacts/trained_model/"

model_nlp = pipeline('question-answering', model=AutoModelForQuestionAnswering.from_pretrained(model_dir), tokenizer=AutoTokenizer.from_pretrained(model_dir))

"""Just some fun analysis to try to get some intuition in what might be going on:

"""

new_context = """Christian Pulisic assured his United States teammates he'd be ready for Saturday's match against the Netherlands after picking up an abdominal injury in his team's Group B match win over Iran on Tuesday.

                Pulisic was involved in a hard collision with Iran keeper Alireza Beiranvand shortly before half-time as he tapped in a headed cross from Sergino Dest to put the US up 1-0 and help earn the Americans a spot in the knockout rounds.

                The Chelsea winger said in a social media post after the game: "I'll be ready Saturday, don't worry," while teammate Weston McKennie said: "I sent him a text and checked on him, and he said, `Best believe I'll be ready on Saturday.'"

                Pulisic lay on the ground in the goalmouth for a few minutes following the collision before leaving the field and receiving attention from the team's trainers and then coming back on to finish the first half.

                Brenden Aaronson came on for Pulisic after half-time, with US Soccer saying after the match he had been taken to the hospital and diagnosed with a pelvic contusion that was termed "day to day."

                "Obviously we're very thankful that he threw his body there,'' McKennie said after the match. "At the end it was a heart-drop sinking moment, but we got it done and we're excited to still be here.''

                US coach Gregg Berhalter praised Pulisic's hard work and toughness after the match."""

print(model_nlp({"context": new_context,"question":"Who did Christian Pulisic crash into"})) # easy, gets it
print(model_nlp({"context": new_context,"question":"Who did Christian Pulisic beat"})) # "beat" is not in the context, isn't generalizing to new data well
print(model_nlp({"context": new_context,"question":"Who did Christian Pulisic win over"}))  # win over in context, but have to go from pulisic -> his team -> over iran, kind of multi-hop ish
print(model_nlp({"context": new_context,"question":"Who did the United States beat"})) # again, more direct question but "beat" is new to the model given the context
print(model_nlp({"context": new_context,"question":"Who did the United States win over"})) # I think the "Who" is trying to identify a person, tricking the model 
print(model_nlp({"context": new_context,"question":"Which team did the United States win over"})) # Answers the super direct, with words from context question correct
print(model_nlp({"context": new_context,"question":"Which team does the United States face next"})) # I think this might be right just because Netherlands appears before Iran
print(model_nlp({"context": new_context,"question":"What injury does Christian Pulisic have"})) # correct
print(model_nlp({"context": new_context,"question":"Did Weston McKennie get hurt"})) # answers the question but for the wrong subject
print(model_nlp({"context": new_context,"question":"Did Christian Pulisic get hurt"})) # kind of indirect answer, but kind of right which is cool
print(model_nlp({"context": new_context,"question":"What teams are playing in the World Cup"})) # can't give multiple answers in this format
print(model_nlp({"context": new_context,"question":"What emotion did Weston McKennie feel"})) # better answer than I expected, but was hoping for excited or thankful
print(model_nlp({"context": new_context,"question":"Which team played against the United States but did not win"})) # too complicated? is it the negation?

print(model_nlp({"context": "Samantha is slower than Jordan.","question":"Who is less slow?"})) # doesn't deal with negation well?
c={"context": "Samantha is slower than Jordan.","question":"Who is less slow?"}
model_nlp(c)['answer']

"""Try to do some checklist generation for real:

"""



def format_squad_with_context(x, pred, conf, label=None, *args, **kwargs):
    c, q = x
    ret = 'C: %s\nQ: %s\n' % (c, q)
    if label is not None:
        ret += 'A: %s\n' % label
    ret += 'P: %s\n' % pred
    return ret

def format_squad(x, pred, conf, label=None, *args, **kwargs):
    
    try:
      c, q, s = x
    except:
      c, q = x
    ret = 'Q: %s\n' % (q)
    if label is not None:
        ret += 'A: %s\n' % label
    ret += 'P: %s\n' % pred
    return ret

import json
def load_squad(fold='validation'):
    answers = []
    data = []
    ids = []
    files = {
        'validation': 'fp-dataset-artifacts/datasets/squad-dev-v2.0.json',
        'train': 'fp-dataset-artifacts/datasets/squad-train-v2.0.json',
        }
    f = json.load(open(files[fold]))
    for t in f['data']:
        for p in t['paragraphs']:
            context = p['context']
            for qa in p['qas']:
                data.append({'passage': context, 'question': qa['question'], 'id': qa['id']})
                if qa["is_impossible"]:
                  answers.append(("true",set([(x['text'], x['answer_start']) for x in qa['plausible_answers']])))
                else:
                  answers.append(("false", set([(x['text'], x['answer_start']) for x in qa['answers']])))
    return data, answers

import pickle
data, answers =  load_squad('validation')
pairs = [(x['passage'], x['question'], x['id']) for x in data]

pairs[0]

nlp = spacy.load("en_core_web_sm")

processed_pairs = []
for pair in pairs[0:10]:
  passage_doc = nlp(pair[0])
  question_doc = nlp(pair[1])
  id = pair[2]
  processed_pairs.append((passage_doc, question_doc, id))

print(processed_pairs[0])
print(pairs[0])

print(len(pairs))

from spacy import displacy

displacy.render(processed_pairs[0][1], style = "ent",jupyter = True)
entities = []
for pair in processed_pairs:
  entities.append((pair[1], [(i, i.label_) for i in pair[1].ents ]))
entities

[pair[1] for pair in processed_pairs]

data[9], answers[9]

data[6], answers[6]

suite = TestSuite()

## Vocabulary 
adj = ['old', 'smart', 'tall', 'young', 'strong', 'short', 'tough', 'cool', 'fast', 'nice', 'small', 'dark', 'wise', 'rich', 'great', 'weak', 'high', 'slow', 'strange', 'clean']
adj = [(x.rstrip('e'), x) for x in adj]

from checklist.pred_wrapper import PredictorWrapper
pred = lambda x: ([model_nlp({"context":x[0], "question":x[1]})['answer']], [model_nlp({"context":x[0], "question":x[1]})['score']])
testing_example = [("Samantha is slower than Jordan.","Who is less slow?")]

def predict_and_score_fun(input):
    
    preds = []
    scores = []
    for i in input:
      context = i[0] 
      question = i[1]
      pred = model_nlp({"context":context,"question":question})
      preds.append(pred['answer'])
      scores.append(pred['score'])
    return (preds, scores)

new_pred = PredictorWrapper.wrap_predict(predict_and_score_fun)

print(predict_and_score_fun(testing_example))

'''
print("first data point in T")
print(t.labels[0])
print(t.data[0])
print("second data point in T")
print(t.labels[1])
print(t.data[1])
'''
import random
random.seed(42)
def checklist_to_squad(name, new_data, current_data=None, num_samples=None):

  ids = []
  titles = []
  contexts = []
  questions = []
  answers = []

  for example_group_idx in range(len(new_data.data)):
    for example_idx in range(len(new_data.data[example_group_idx])):
      #print("our indices: ", example_group_idx, example_idx, "in ", len(new_data.data), len(example))
      example_data = new_data.data[example_group_idx][example_idx]
      context, question = example_data
      example_label = new_data.labels[example_group_idx][example_idx]
      ids.append("a"+str(example_group_idx)+str(example_idx))
      titles.append("name")
      contexts.append(context)
      questions.append(question)

      answers.append({'text':[example_label], "answer_start":[context.find(example_label)]})

  my_data = {
      "id": ids,
      "title": titles,
      "context": contexts,
      "question": questions,
      "answers": answers
    
  }
  checklist_data = Dataset.from_dict(my_data, features=squad["train"].features)
  if num_samples != None:
    selection = random.sample(range(checklist_data.num_rows), num_samples)
    checklist_data = checklist_data.select(selection)
  if current_data == None:
    return checklist_data
  else:
    return datasets.concatenate_datasets([current_data, checklist_data])

#checklist_data = checklist_to_squad("Profession_vs_nationality", t)
#print(checklist_data)

t = editor.template(
    [(
    '{first_name} is {adj[0]}er than {first_name1}.',
    'Who is less {adj[1]}?'
    ),(
    '{first_name} is {adj[0]}er than {first_name1}.',
    'Who is {adj[0]}er?'
    )
    ],
    labels = ['{first_name1}','{first_name}'],
    adj=adj,
    remove_duplicates=True,
    nsamples=50,
    save=True
    )
name = 'A is COMP than B. Who is more / less COMP?'
description = ''
test = MFT(**t, name=name, description=description, capability='Vocabulary')
test.run(predict_and_score_fun)
test.summary(n=3, format_example_fn=format_squad_with_context)
suite.add(test, overwrite=True)
checklist_data = checklist_to_squad("more_less_comp", t)
checklist_data_full = checklist_to_squad("more_less_comp", t)

checklist_data

def crossproduct(t):
    # takes the output of editor.template and does the cross product of contexts and qas
    ret = []
    ret_labels = []
    for x in t.data:
        cs = x['contexts']
        qas = x['qas']
        d = list(itertools.product(cs, qas))
        ret.append([(x[0], x[1][0]) for x in d])
        ret_labels.append([x[1][1] for x in d])
    t.data = ret
    t.labels = ret_labels
    return t

def clean(string):
    return string.lstrip('[a,the,an,in,at] ').rstrip('.')

def expect_squad(x, pred, conf, label=None, meta=None):
    return clean(pred) == clean(label)
expect_squad = Expect.single(expect_squad)

state = editor.suggest('John is very {mask} about the project.')[:20]
print(', '.join(editor.suggest('John is {mask} {state} about the project.', state=state)[:30]))
very = ['very', 'extremely', 'really', 'quite', 'incredibly', 'particularly', 'highly', 'super']
somewhat = ['a little', 'somewhat', 'slightly', 'mildly']

t = crossproduct(editor.template(
    {
        'contexts': [
            '{first_name} is {very} {s} about the project. {first_name1} is {s} about the project.',
            '{first_name1} is {s} about the project. {first_name} is {very} {s} about the project.',
            '{first_name} is {s} about the project. {first_name1} is {somewhat} {s} about the project.',
            '{first_name1} is {somewhat} {s} about the project. {first_name} is {s} about the project.',
            '{first_name} is {very} {s} about the project. {first_name1} is {somewhat} {s} about the project.',
            '{first_name1} is {somewhat} {s} about the project. {first_name} is {very} {s} about the project.',
        ],
        'qas': [
            (
                'Who is most {s} about the project?',
                '{first_name}'
            ), 
            (
                'Who is least {s} about the project?',
                '{first_name1}'
            ), 
            
        ]
        
    },
    s = state,
    very=very,
    somewhat=somewhat,
    remove_duplicates=True,
    nsamples=50,
    save=True
    ))
name = 'Intensifiers (very, super, extremely) and reducers (somewhat, kinda, etc)?'
desc = ''
test = MFT(**t, name=name, description=desc, capability='Vocabulary')
test.run(predict_and_score_fun)
test.summary(n=3, format_example_fn=format_squad_with_context)
suite.add(test, overwrite=True)
checklist_data = checklist_to_squad("intensifiers_and_reducers", t, checklist_data, 100)
checklist_data_full = checklist_to_squad("intensifiers_and_reducers", t, checklist_data_full)

checklist_data

model_nlp({'context': "Samuel is very particular about the project. Keith is somewhat particular about the project.", "question":"Who is most particular about the project?"})

animals = ['dog', 'cat', 'bull', 'cow', 'fish', 'serpent', 'snake', 'lizard', 'hamster', 'rabbit', 'guinea pig', 'iguana', 'duck']
vehicles = ['car', 'truck', 'train', 'motorcycle', 'bike', 'firetruck', 'tractor', 'van', 'SUV', 'minivan']
t = crossproduct(editor.template(
    {
        'contexts': [
            '{first_name} has {a:animal} and {a:vehicle}.',
            '{first_name} has {a:vehicle} and {a:animal}.',
        ],
        'qas': [
            (
                'What animal does {first_name} have?',
                '{animal}'
            ), 
            (
                'What vehicle does {first_name} have?',
                '{vehicle}'
            ), 
            
        ]
        
    },
    animal=animals,
    vehicle=vehicles,
    remove_duplicates=True,
    nsamples=50,
    save=True
    ))
name = 'Animal vs Vehicle'
test = MFT(**t, name=name, description='', capability='Taxonomy', expect=expect_squad)
test.run(predict_and_score_fun)
test.summary(n=3, format_example_fn=format_squad_with_context)
suite.add(test, overwrite=True)
checklist_data = checklist_to_squad("animal_vs_vehicle", t, checklist_data, 100)
checklist_data_full = checklist_to_squad("animal_vs_vehicle", t, checklist_data_full)

professions = editor.suggest('{first_name} works as {a:mask}.')[:30]
professions += editor.suggest('{first_name} {last_name} works as {a:mask}.')[:30]
professions = list(set(professions))
if 'translator' in professions:
    professions.remove('translator')

t = crossproduct(editor.template(
    {
        'contexts': [
            '{first_name} is {a:nat} {prof}.',
            '{first_name} is {a:prof}. {first_name} is {nat}.',
            '{first_name} is {nat}. {first_name} is {a:prof}.',
            '{first_name} is {nat} and {a:prof}.',
            '{first_name} is {a:prof} and {nat}.',
        ],
        'qas': [
            (
                'What is {first_name}\'s job?',
                '{prof}'
            ), 
            (
                'What is {first_name}\'s nationality?',
                '{nat}'
            ), 
            
        ]
        
    },
    nat = editor.lexicons['nationality'][:10],
    prof=professions,
    remove_duplicates=True,
    nsamples=500,
    save=True,
    ))
name = 'Profession vs nationality'
test = MFT(**t, name=name, expect=expect_squad, description='',  capability='Taxonomy')
test.run(predict_and_score_fun)
test.summary(n=3, format_example_fn=format_squad_with_context)
suite.add(test, overwrite=True)
checklist_data = checklist_to_squad("profession_vs_nationality", t, checklist_data, 100)
checklist_data_full = checklist_to_squad("profession_vs_nationality", t, checklist_data_full)



comp_pairs = [('better', 'worse'), ('older', 'younger'), ('smarter', 'dumber'), ('taller', 'shorter'), ('bigger', 'smaller'), ('stronger', 'weaker'), ('faster', 'slower'), ('darker', 'lighter'), ('richer', 'poorer'), ('happier', 'sadder'), ('louder', 'quieter'), ('warmer', 'colder')]
comp_pairs = list(set(comp_pairs))#list(set(comp_pairs + [(x[1], x[0]) for x in comp_pairs]))

t = crossproduct(editor.template(
    {
        'contexts': [
            '{first_name} is {comp[0]} than {first_name1}.',
            '{first_name1} is {comp[1]} than {first_name}.',
        ],
        'qas': [
            (
                'Who is {comp[1]}?',
                '{first_name1}',
            ),
            (
                'Who is {comp[0]}?',
                '{first_name}',
            )
            
        ]
        ,
    },
    comp=comp_pairs,
    remove_duplicates=True,
    nsamples=50,
    save=True
    ))
name = 'A is COMP than B. Who is antonym(COMP)? B'
test = MFT(**t, name=name, description='', capability='Taxonomy')
test.run(predict_and_score_fun)
test.summary(n=3, format_example_fn=format_squad_with_context)
suite.add(test, overwrite=True)
checklist_data = checklist_to_squad("comp_pairs", t, checklist_data, 100)
checklist_data_full = checklist_to_squad("comp_pairs", t, checklist_data_full)

model_nlp({'context': "Maria is happier than Carol.", "question":"Who is sadder?"})

#context, question = t2.data[0][1]

'''
def question_typo(x):
    return (x[0], Perturb.add_typos(x[1]))
t2 = Perturb.perturb(pairs, question_typo, nsamples=50)
test = INV(**t2, name='Question typo', capability='Robustness', description='')
test.run(predict_and_score_fun)
test.summary(n=3, format_example_fn=format_squad)
suite.add(test, overwrite=True)
checklist_data = checklist_to_squad("typos", t2, checklist_data)
'''



squad



import re
def change_thing(change_fn):
    def change_both(cq, **kwargs):
        context, question = cq
        a = change_fn(context, meta=True)
        if not a:
            return None
        changed, meta = a
        ret = []
        for c, m in zip(changed, meta):
            new_q = re.sub(r'\b%s\b' % re.escape(m[0]), m[1], question.text)
            ret.append((c, new_q))
        return ret, meta
    return change_both

def expect_same(orig_pred, pred, orig_conf, conf, labels=None, meta=None):
    if not meta:
        return pred == orig_pred
    return pred == re.sub(r'\b%s\b' % re.escape(meta[0]), meta[1], orig_pred)

def format_replace(x, pred, conf, label=None, meta=None):
    ret = format_squad(x, pred, conf, label, meta)
    if meta:
        ret += 'Perturb: %s -> %s\n' % meta
    return ret

def format_replace_context(x, pred, conf, label=None, meta=None):
    ret = format_squad_with_context(x, pred, conf, label, meta)
    if meta:
        ret += 'Perturb: %s -> %s\n' % meta
    return ret





!pip install jsonlines
import jsonlines
import pandas as pd

columns = ['id', 'title','context','pred','question','answers_written','answers_idx', 'exact_match']
answers_df = pd.DataFrame(columns=columns)

with jsonlines.open('/content/fp-dataset-artifacts/baseline_eval/eval_predictions.jsonl') as reader:
  for obj in reader.iter(type=dict, skip_invalid=True):    
    id = obj['id']
    title = obj['title']
    context = obj['context']
    pred = obj['predicted_answer']
    question = obj['question']
    answers_written = obj['answers']['text']
    answers_idx = obj['answers']['answer_start']
    answer_exact = obj['exact_match']
    entry = [id, title, context, pred, question, answers_written, answers_idx, answer_exact]
    answers_df.loc[len(answers_df)] = entry

answers_df[['title','exact_match']].groupby('title').agg({i:'value_counts' for i in answers_df[['title','exact_match']].columns[1:]}).groupby(level=0).transform(lambda x: x.div(x.sum()))

answers_df[answers_df['question'].str.contains("Who")].groupby('exact_match').count() # 1040 / 1240 = 83.8%

answers_df[answers_df['question'].str.contains("Which")].groupby('exact_match').count() # 353/454 = 77.75%

answers_df[answers_df['question'].str.contains("Why")].groupby('exact_match').count() # 88/150 = 58.66666%

answers_df[answers_df['answers_idx'].apply(lambda x:x[0]) / answers_df.context.str.len() <= 0.25].groupby('exact_match').count() # 2999 / 3710 = 80.8%

answers_df[answers_df['answers_idx'].apply(lambda x:x[0]) / answers_df.context.str.len() >= 0.75].groupby('exact_match').count() # 1556 / 2014 = 77.26%

answers_df[(answers_df['answers_idx'].apply(lambda x:x[0]) / answers_df.context.str.len() > 0.25) & (answers_df['answers_idx'].apply(lambda x:x[0]) / answers_df.context.str.len() < 0.75)].groupby('exact_match').count() # 3770 / 4846 = 77.80%

mask = answers_df['question'].str.contains(r'[0-9]')
answers_df[mask].groupby('exact_match').count() # 1035 / 1301 = 79.55%

answers_df[~mask].groupby('exact_match').count() # 7290 / 9269 = 78.65%

answers_df[answers_df['question'].str.contains("not")].groupby('exact_match').count() # 278 / 375 = 74.13%

answers_df.groupby('exact_match').count()



squad

import random
random.seed(42)
squad_adv_selection = random.sample(range(squad_adv_addSent['validation'].num_rows), 500)
nq_selection = random.sample(range(2356), 400)
squad_selection = random.sample(range(10570), 500)
squad_v2_selection = random.sample(range(squad_v2['train'].num_rows), 500)

squad_adv_addSent_limited = squad_adv_addSent['validation'].select(squad_adv_selection)
nq_data_unformatted = nq_data['train']
squad_limited = squad['train'].select(squad_selection)
squad_v2_limited = squad_v2['train'].select(squad_v2_selection)

checklist_data

squad_adv_addSent['validation'][0]

nq_data_unformatted['paragraphs'][0:1]

from datasets import Dataset
import numpy as np
ids = []
titles = []
contexts = []
questions = []
answers = []

for example in nq_data_unformatted['paragraphs']:
  qas = example[0]['qas'][0]
  if qas['is_impossible'] == 'false' or qas['is_impossible'] == False:
    contexts.append(example[0]['context'])
    qas = example[0]['qas'][0]
    questions.append(qas['question'])
    #answers.append(qas['answers'])
    titles.append("natural_questioning")
    ids.append(qas['id'])
    answers_reshaped = {}
    texts = []
    answer_starts = []

    for ans in qas['answers']:
      #print("cuurent ans is: ", ans)
      if type(ans) == list:
        #print("found a list")
        for a in ans:
          #print("current a is: ",a)
          try:
            texts.append(a['text'])
            answer_starts.append(a['answer_start'])
          except:
            pass
      else:
        try:
          texts.append(ans['text'])
          answer_starts.append(ans['answer_start'])
        except:
          pass
    #print("adding: ",{"answer_start": answer_starts, "text": texts})
    answers.append({"answer_start": answer_starts, "text": texts})


my_data = {
    "id": ids,
    "title": titles,
    "context": contexts,
    "question": questions,
    "answers": answers
  
}

#print(my_data['answers'])
#print(my_data)  
nq_data_formatted = Dataset.from_dict(my_data, features=squad["train"].features)
nq_data_limited = nq_data_formatted.select(nq_selection)

nq_data_limited[0]

nq_data_formatted.save_to_disk("./fp-dataset-artifacts/datasets/nq_data_formatted.json")

ids = []
titles = []
contexts = []
questions = []
answers = []

with jsonlines.open('/content/fp-dataset-artifacts/datasets/squad_handmade.jsonl') as reader:
  for obj in reader.iter(type=dict, skip_invalid=True):
    ids.append(obj['id'])
    titles.append(obj['title'])
    contexts.append(obj['context'])
    questions.append(obj['question'])
    answers.append(obj['answers'])

my_data = {
    "id": ids,
    "title": titles,
    "context": contexts,
    "question": questions,
    "answers": answers
  
}

manual_data = Dataset.from_dict(my_data, features=squad["train"].features)



import numpy as np
import collections
from collections import defaultdict, OrderedDict
from transformers import Trainer, EvalPrediction
from transformers.trainer_utils import PredictionOutput
from typing import Tuple
from tqdm.auto import tqdm

QA_MAX_ANSWER_LENGTH = 30


# This function preprocesses an NLI dataset, tokenizing premises and hypotheses.
def prepare_dataset_nli(examples, tokenizer, max_seq_length=None):
    max_seq_length = tokenizer.model_max_length if max_seq_length is None else max_seq_length

    tokenized_examples = tokenizer(
        examples['premise'],
        examples['hypothesis'],
        truncation=True,
        max_length=max_seq_length,
        padding='max_length'
    )

    tokenized_examples['label'] = examples['label']
    return tokenized_examples


# This function computes sentence-classification accuracy.
# Functions with signatures like this one work as the "compute_metrics" argument of transformers.Trainer.
def compute_accuracy(eval_preds: EvalPrediction):
    return {
        'accuracy': (np.argmax(
            eval_preds.predictions,
            axis=1) == eval_preds.label_ids).astype(
            np.float32).mean().item()
    }


# This function preprocesses a question answering dataset, tokenizing the question and context text
# and finding the right offsets for the answer spans in the tokenized context (to use as labels).
# Adapted from https://github.com/huggingface/transformers/blob/master/examples/pytorch/question-answering/run_qa.py
def prepare_train_dataset_qa(examples, tokenizer, max_seq_length=None):
    questions = [q.lstrip() for q in examples["question"]]
    max_seq_length = tokenizer.model_max_length
    # tokenize both questions and the corresponding context
    # if the context length is longer than max_length, we split it to several
    # chunks of max_length
    tokenized_examples = tokenizer(
        questions,
        examples["context"],
        truncation="only_second",
        max_length=max_seq_length,
        stride=min(max_seq_length // 2, 128),
        return_overflowing_tokens=True,
        return_offsets_mapping=True,
        padding="max_length"
    )

    # Since one example might give us several features if it has a long context,
    # we need a map from a feature to its corresponding example.
    sample_mapping = tokenized_examples.pop("overflow_to_sample_mapping")
    # The offset mappings will give us a map from token to character position
    # in the original context. This will help us compute the start_positions
    # and end_positions to get the final answer string.
    offset_mapping = tokenized_examples.pop("offset_mapping")

    tokenized_examples["start_positions"] = []
    tokenized_examples["end_positions"] = []

    for i, offsets in enumerate(offset_mapping):
        input_ids = tokenized_examples["input_ids"][i]
        # We will label features not containing the answer the index of the CLS token.
        cls_index = input_ids.index(tokenizer.cls_token_id)
        sequence_ids = tokenized_examples.sequence_ids(i)
        # from the feature idx to sample idx
        sample_index = sample_mapping[i]
        # get the answer for a feature
        answers = examples["answers"][sample_index]

        if len(answers["answer_start"]) == 0:
            tokenized_examples["start_positions"].append(cls_index)
            tokenized_examples["end_positions"].append(cls_index)
        else:
            # Start/end character index of the answer in the text.
            start_char = answers["answer_start"][0]
            end_char = start_char + len(answers["text"][0])

            # Start token index of the current span in the text.
            token_start_index = 0
            while sequence_ids[token_start_index] != 1:
                token_start_index += 1

            # End token index of the current span in the text.
            token_end_index = len(input_ids) - 1
            while sequence_ids[token_end_index] != 1:
                token_end_index -= 1

            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).
            if not (offsets[token_start_index][0] <= start_char and
                    offsets[token_end_index][1] >= end_char):
                tokenized_examples["start_positions"].append(cls_index)
                tokenized_examples["end_positions"].append(cls_index)
            else:
                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.
                # Note: we could go after the last offset if the answer is the last word (edge case).
                while token_start_index < len(offsets) and \
                        offsets[token_start_index][0] <= start_char:
                    token_start_index += 1
                tokenized_examples["start_positions"].append(
                    token_start_index - 1)
                while offsets[token_end_index][1] >= end_char:
                    token_end_index -= 1
                tokenized_examples["end_positions"].append(token_end_index + 1)

    return tokenized_examples


def prepare_validation_dataset_qa(examples, tokenizer):
    questions = [q.lstrip() for q in examples["question"]]
    max_seq_length = tokenizer.model_max_length
    tokenized_examples = tokenizer(
        questions,
        examples["context"],
        truncation="only_second",
        max_length=max_seq_length,
        stride=min(max_seq_length // 2, 128),
        return_overflowing_tokens=True,
        return_offsets_mapping=True,
        padding="max_length"
    )

    # Since one example might give us several features if it has a long context, we need a map from a feature to
    # its corresponding example. This key gives us just that.
    sample_mapping = tokenized_examples.pop("overflow_to_sample_mapping")

    # For evaluation, we will need to convert our predictions to substrings of the context, so we keep the
    # corresponding example_id and we will store the offset mappings.
    tokenized_examples["example_id"] = []

    for i in range(len(tokenized_examples["input_ids"])):
        # Grab the sequence corresponding to that example (to know what is the context and what is the question).
        sequence_ids = tokenized_examples.sequence_ids(i)
        context_index = 1

        # One example can give several spans, this is the index of the example containing this span of text.
        sample_index = sample_mapping[i]
        tokenized_examples["example_id"].append(examples["id"][sample_index])

        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token
        # position is part of the context or not.
        tokenized_examples["offset_mapping"][i] = [
            (o if sequence_ids[k] == context_index else None)
            for k, o in enumerate(tokenized_examples["offset_mapping"][i])
        ]

    return tokenized_examples


# This function uses start and end position scores predicted by a question answering model to
# select and extract the predicted answer span from the context.
# Adapted from https://github.com/huggingface/transformers/blob/master/examples/pytorch/question-answering/utils_qa.py
def postprocess_qa_predictions(examples,
                               features,
                               predictions: Tuple[np.ndarray, np.ndarray],
                               n_best_size: int = 20):
    if len(predictions) != 2:
        raise ValueError(
            "`predictions` should be a tuple with two elements (start_logits, end_logits).")
    all_start_logits, all_end_logits = predictions

    if len(predictions[0]) != len(features):
        raise ValueError(
            f"Got {len(predictions[0])} predictions and {len(features)} features.")

    # Build a map example to its corresponding features.
    example_id_to_index = {k: i for i, k in enumerate(examples["id"])}
    features_per_example = collections.defaultdict(list)
    for i, feature in enumerate(features):
        features_per_example[
            example_id_to_index[feature["example_id"]]].append(i)

    # The dictionaries we have to fill.
    all_predictions = collections.OrderedDict()

    # Let's loop over all the examples!
    for example_index, example in enumerate(tqdm(examples)):
        # Those are the indices of the features associated to the current example.
        feature_indices = features_per_example[example_index]

        prelim_predictions = []

        # Looping through all the features associated to the current example.
        for feature_index in feature_indices:
            # We grab the predictions of the model for this feature.
            start_logits = all_start_logits[feature_index]
            end_logits = all_end_logits[feature_index]
            # This is what will allow us to map some the positions in our logits
            # to span of texts in the original context.
            offset_mapping = features[feature_index]["offset_mapping"]

            # Go through all possibilities for the `n_best_size` greater start and end logits.
            start_indexes = np.argsort(start_logits)[
                            -1: -n_best_size - 1: -1].tolist()
            end_indexes = np.argsort(end_logits)[
                          -1: -n_best_size - 1: -1].tolist()
            for start_index in start_indexes:
                for end_index in end_indexes:
                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond
                    # to part of the input_ids that are not in the context.
                    if (
                            start_index >= len(offset_mapping)
                            or end_index >= len(offset_mapping)
                            or offset_mapping[start_index] is None
                            or offset_mapping[end_index] is None
                    ):
                        continue
                    # Don't consider answers with a length that is either < 0 or > max_answer_length.
                    if end_index < start_index or \
                            end_index - start_index + 1 > QA_MAX_ANSWER_LENGTH:
                        continue

                    prelim_predictions.append(
                        {
                            "offsets": (offset_mapping[start_index][0],
                                        offset_mapping[end_index][1]),
                            "score": start_logits[start_index] +
                                     end_logits[end_index],
                            "start_logit": start_logits[start_index],
                            "end_logit": end_logits[end_index],
                        }
                    )

        # Only keep the best `n_best_size` predictions.
        predictions = sorted(prelim_predictions, key=lambda x: x["score"],
                             reverse=True)[:n_best_size]

        # Use the offsets to gather the answer text in the original context.
        context = example["context"]
        for pred in predictions:
            offsets = pred.pop("offsets")
            pred["text"] = context[offsets[0]: offsets[1]]

        # In the very rare edge case we have not a single non-null prediction,
        # we create a fake prediction to avoid failure.
        if len(predictions) == 0 or (
                len(predictions) == 1 and predictions[0]["text"] == ""):
            predictions.insert(0, {"text": "empty", "start_logit": 0.0,
                                   "end_logit": 0.0, "score": 0.0})

        all_predictions[example["id"]] = predictions[0]["text"]
    return all_predictions


# Adapted from https://github.com/huggingface/transformers/blob/master/examples/pytorch/question-answering/trainer_qa.py
class QuestionAnsweringTrainer(Trainer):
    def __init__(self, *args, eval_examples=None, **kwargs):
        super().__init__(*args, **kwargs)
        self.eval_examples = eval_examples

    def evaluate(self,
                 eval_dataset=None,  # denotes the dataset after mapping
                 eval_examples=None,  # denotes the raw dataset
                 ignore_keys=None,  # keys to be ignored in dataset
                 metric_key_prefix: str = "eval"
                 ):
        eval_dataset = self.eval_dataset if eval_dataset is None else eval_dataset
        eval_dataloader = self.get_eval_dataloader(eval_dataset)
        eval_examples = self.eval_examples if eval_examples is None else eval_examples

        # Temporarily disable metric computation, we will do it in the loop here.
        compute_metrics = self.compute_metrics
        self.compute_metrics = None
        try:
            # compute the raw predictions (start_logits and end_logits)
            output = self.evaluation_loop(
                eval_dataloader,
                description="Evaluation",
                # No point gathering the predictions if there are no metrics, otherwise we defer to
                # self.args.prediction_loss_only
                prediction_loss_only=True if compute_metrics is None else None,
                ignore_keys=ignore_keys,
            )
        finally:
            self.compute_metrics = compute_metrics

        if self.compute_metrics is not None:
            # post process the raw predictions to get the final prediction
            # (from start_logits, end_logits to an answer string)
            eval_preds = postprocess_qa_predictions(eval_examples,
                                                    eval_dataset,
                                                    output.predictions)
            formatted_predictions = [{"id": k, "prediction_text": v}
                                     for k, v in eval_preds.items()]
            references = [{"id": ex["id"], "answers": ex['answers']}
                          for ex in eval_examples]

            # compute the metrics according to the predictions and references
            metrics = self.compute_metrics(
                EvalPrediction(predictions=formatted_predictions,
                               label_ids=references)
            )

            # Prefix all keys with metric_key_prefix + '_'
            for key in list(metrics.keys()):
                if not key.startswith(f"{metric_key_prefix}_"):
                    metrics[f"{metric_key_prefix}_{key}"] = metrics.pop(key)

            self.log(metrics)
        else:
            metrics = {}

        self.control = self.callback_handler.on_evaluate(self.args, self.state,
                                                         self.control, metrics)
        return metrics

from transformers import Trainer

squad_adv_addSent_limited

checklist_data

combined_df = datasets.concatenate_datasets([squad_limited, manual_data, checklist_data])

manual_data

# set baseline model for training start point
model_dir = "./fp-dataset-artifacts/trained_model/" # "./tmp_trainer/" # 
baseline_model = AutoModelForQuestionAnswering.from_pretrained(model_dir)
tokenizer = AutoTokenizer.from_pretrained(model_dir)
baseline_model_pipeline = pipeline('question-answering', model=baseline_model, tokenizer=tokenizer)

# Dataset selection
# In this chunk, we want to use the random subset of the adversarial squad dataset
dataset = combined_df # manual_data # squad_limited checklist_data combined_df  nq_data_limited squad_adv_addSent_limited

# Select the dataset preprocessing function (these functions are defined in helpers.py)
prepare_train_dataset = lambda exs: prepare_train_dataset_qa(exs, tokenizer)
prepare_eval_dataset = lambda exs: prepare_validation_dataset_qa(exs, tokenizer)


train_dataset = None
#eval_dataset = None
train_dataset_featurized = None
#eval_dataset_featurized = None

NUM_PREPROCESSING_WORKERS = 2

train_dataset = dataset
train_dataset_featurized = train_dataset.map(
    prepare_train_dataset,
    batched=True,
    num_proc=NUM_PREPROCESSING_WORKERS,
    remove_columns=train_dataset.column_names
    )

# Select the training configuration
trainer_class = Trainer
eval_kwargs = {}
# If you want to use custom metrics, you should define your own "compute_metrics" function.
# For an example of a valid compute_metrics function, see compute_accuracy in helpers.py.
compute_metrics = None

# For QA, we need to use a tweaked version of the Trainer (defined in helpers.py)
# to enable the question-answering specific evaluation metrics
trainer_class = QuestionAnsweringTrainer
eval_kwargs['eval_examples'] = train_dataset
metric = datasets.load_metric('squad')
compute_metrics = lambda eval_preds: metric.compute(
    predictions=eval_preds.predictions, references=eval_preds.label_ids)

# This function wraps the compute_metrics function, storing the model's predictions
# so that they can be dumped along with the computed metrics
eval_predictions = None
def compute_metrics_and_store_predictions(eval_preds):
    eval_predictions = eval_preds
    return compute_metrics(eval_preds)

# Initialize the Trainer object with the specified arguments and the model and dataset we loaded above
trainer = trainer_class(
    model=baseline_model,
    train_dataset=train_dataset_featurized,
    eval_dataset=train_dataset_featurized,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics_and_store_predictions
)
# Train and/or evaluate

trainer.train()
trainer.save_model()

print("done")



tuned_model_dir = "./fp-dataset-artifacts/tuned_model/"

tuned_model_nlp = pipeline('question-answering', model=AutoModelForQuestionAnswering.from_pretrained(model_dir), tokenizer=AutoTokenizer.from_pretrained(model_dir))



columns = ['id', 'title','context','pred','question','answers_written','answers_idx', 'exact_match']
answers_df_tuned_combined = pd.DataFrame(columns=columns)

with jsonlines.open('/content/fp-dataset-artifacts/tuned_eval/eval_predictions.jsonl') as reader:
  for obj in reader.iter(type=dict, skip_invalid=True):    
    id = obj['id']
    title = obj['title']
    context = obj['context']
    pred = obj['predicted_answer']
    question = obj['question']
    answers_written = obj['answers']['text']
    answers_idx = obj['answers']['answer_start']
    answer_exact = obj['exact_match']
    entry = [id, title, context, pred, question, answers_written, answers_idx, answer_exact]
    answers_df_tuned_combined.loc[len(answers_df_tuned_combined)] = entry

answers_df_tuned_combined.groupby('exact_match').count()

answers_df_tuned_combined[answers_df_tuned_combined['question'].str.contains("Who")].groupby('exact_match').count() # 1024 / 1240 = 82.6%

answers_df_tuned_combined[answers_df_tuned_combined['question'].str.contains("Which")].groupby('exact_match').count() # 339 / 454 = 74.7

answers_df_tuned_combined[answers_df_tuned_combined['question'].str.contains("Why")].groupby('exact_match').count() # 81 / 150 = 54

answers_df_tuned_combined[answers_df_tuned_combined['question'].str.contains("not")].groupby('exact_match').count() # 265 / 375 = 70.7%

answers_df_tuned_combined[answers_df_tuned_combined['answers_idx'].apply(lambda x:x[0]) / answers_df_tuned_combined.context.str.len() <= 0.25].groupby('exact_match').count() # 2942 / 3710 = 79.3%

answers_df_tuned_combined[answers_df_tuned_combined['answers_idx'].apply(lambda x:x[0]) / answers_df_tuned_combined.context.str.len() >= 0.75].groupby('exact_match').count() # 1515 / 2014 = 75.2



suite_tuned = TestSuite()

tuned_model_dir = "./tmp_trainer/"

tuned_model_nlp = pipeline('question-answering', model=AutoModelForQuestionAnswering.from_pretrained(model_dir), tokenizer=AutoTokenizer.from_pretrained(model_dir))

def predict_and_score_fun(input):
    
    preds = []
    scores = []
    for i in input:
      context = i[0] 
      question = i[1]
      pred = tuned_model_nlp({"context":context,"question":question})
      preds.append(pred['answer'])
      scores.append(pred['score'])
    return (preds, scores)

## Vocabulary 
adj = ['old', 'smart', 'tall', 'young', 'strong', 'short', 'tough', 'cool', 'fast', 'nice', 'small', 'dark', 'wise', 'rich', 'great', 'weak', 'high', 'slow', 'strange', 'clean']
adj = [(x.rstrip('e'), x) for x in adj]

t = editor.template(
    [(
    '{first_name} is {adj[0]}er than {first_name1}.',
    'Who is less {adj[1]}?'
    ),(
    '{first_name} is {adj[0]}er than {first_name1}.',
    'Who is {adj[0]}er?'
    )
    ],
    labels = ['{first_name1}','{first_name}'],
    adj=adj,
    remove_duplicates=True,
    nsamples=50,
    save=True
    )
name = 'A is COMP than B. Who is more / less COMP?'
description = ''
test = MFT(**t, name=name, description=description, capability='Vocabulary')
test.run(predict_and_score_fun)
test.summary(n=3, format_example_fn=format_squad_with_context)
suite_tuned.add(test, overwrite=True)

state = editor.suggest('John is very {mask} about the project.')[:20]
very = ['very', 'extremely', 'really', 'quite', 'incredibly', 'particularly', 'highly', 'super']
somewhat = ['a little', 'somewhat', 'slightly', 'mildly']

t = crossproduct(editor.template(
    {
        'contexts': [
            '{first_name} is {very} {s} about the project. {first_name1} is {s} about the project.',
            '{first_name1} is {s} about the project. {first_name} is {very} {s} about the project.',
            '{first_name} is {s} about the project. {first_name1} is {somewhat} {s} about the project.',
            '{first_name1} is {somewhat} {s} about the project. {first_name} is {s} about the project.',
            '{first_name} is {very} {s} about the project. {first_name1} is {somewhat} {s} about the project.',
            '{first_name1} is {somewhat} {s} about the project. {first_name} is {very} {s} about the project.',
        ],
        'qas': [
            (
                'Who is most {s} about the project?',
                '{first_name}'
            ), 
            (
                'Who is least {s} about the project?',
                '{first_name1}'
            ), 
            
        ]
        
    },
    s = state,
    very=very,
    somewhat=somewhat,
    remove_duplicates=True,
    nsamples=50,
    save=True
    ))
name = 'Intensifiers (very, super, extremely) and reducers (somewhat, kinda, etc)?'
desc = ''
test = MFT(**t, name=name, description=desc, capability='Vocabulary')
test.run(predict_and_score_fun)
test.summary(n=3, format_example_fn=format_squad_with_context)
suite_tuned.add(test, overwrite=True)

animals = ['dog', 'cat', 'bull', 'cow', 'fish', 'serpent', 'snake', 'lizard', 'hamster', 'rabbit', 'guinea pig', 'iguana', 'duck']
vehicles = ['car', 'truck', 'train', 'motorcycle', 'bike', 'firetruck', 'tractor', 'van', 'SUV', 'minivan']
t = crossproduct(editor.template(
    {
        'contexts': [
            '{first_name} has {a:animal} and {a:vehicle}.',
            '{first_name} has {a:vehicle} and {a:animal}.',
        ],
        'qas': [
            (
                'What animal does {first_name} have?',
                '{animal}'
            ), 
            (
                'What vehicle does {first_name} have?',
                '{vehicle}'
            ), 
            
        ]
        
    },
    animal=animals,
    vehicle=vehicles,
    remove_duplicates=True,
    nsamples=50,
    save=True
    ))
name = 'Animal vs Vehicle'
test = MFT(**t, name=name, description='', capability='Taxonomy', expect=expect_squad)
test.run(predict_and_score_fun)
test.summary(n=3, format_example_fn=format_squad_with_context)
suite_tuned.add(test, overwrite=True)

professions = editor.suggest('{first_name} works as {a:mask}.')[:30]
professions += editor.suggest('{first_name} {last_name} works as {a:mask}.')[:30]
professions = list(set(professions))
if 'translator' in professions:
    professions.remove('translator')

t = crossproduct(editor.template(
    {
        'contexts': [
            '{first_name} is {a:nat} {prof}.',
            '{first_name} is {a:prof}. {first_name} is {nat}.',
            '{first_name} is {nat}. {first_name} is {a:prof}.',
            '{first_name} is {nat} and {a:prof}.',
            '{first_name} is {a:prof} and {nat}.',
        ],
        'qas': [
            (
                'What is {first_name}\'s job?',
                '{prof}'
            ), 
            (
                'What is {first_name}\'s nationality?',
                '{nat}'
            ), 
            
        ]
        
    },
    nat = editor.lexicons['nationality'][:10],
    prof=professions,
    remove_duplicates=True,
    nsamples=500,
    save=True,
    ))
name = 'Profession vs nationality'
test = MFT(**t, name=name, expect=expect_squad, description='',  capability='Taxonomy')
test.run(predict_and_score_fun)
test.summary(n=3, format_example_fn=format_squad_with_context)
suite_tuned.add(test, overwrite=True)

comp_pairs = [('better', 'worse'), ('older', 'younger'), ('smarter', 'dumber'), ('taller', 'shorter'), ('bigger', 'smaller'), ('stronger', 'weaker'), ('faster', 'slower'), ('darker', 'lighter'), ('richer', 'poorer'), ('happier', 'sadder'), ('louder', 'quieter'), ('warmer', 'colder')]
comp_pairs = list(set(comp_pairs))#list(set(comp_pairs + [(x[1], x[0]) for x in comp_pairs]))

t = crossproduct(editor.template(
    {
        'contexts': [
            '{first_name} is {comp[0]} than {first_name1}.',
            '{first_name1} is {comp[1]} than {first_name}.',
        ],
        'qas': [
            (
                'Who is {comp[1]}?',
                '{first_name1}',
            ),
            (
                'Who is {comp[0]}?',
                '{first_name}',
            )
            
        ]
        ,
    },
    comp=comp_pairs,
    remove_duplicates=True,
    nsamples=50,
    save=True
    ))
name = 'A is COMP than B. Who is antonym(COMP)? B'
test = MFT(**t, name=name, description='', capability='Taxonomy')
test.run(predict_and_score_fun)
test.summary(n=3, format_example_fn=format_squad_with_context)
suite_tuned.add(test, overwrite=True)







